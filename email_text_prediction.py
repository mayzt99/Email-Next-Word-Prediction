# -*- coding: utf-8 -*-
"""streamlit_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NUpZ-A7_GwcSieK2w8N4gn-w3r8mV33a
"""

import streamlit as st
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Embedding, LSTM, Input, Dense

from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import joblib
import pickle


st.markdown(
    """
    <style>
        .user-icon-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100px; /* Adjust the height as needed */
        }
        .user-icon img {
            width: 100px; /* Adjust the width as needed */
            height: 100px; /* Adjust the height as needed */
        }
    </style>
    """,
    unsafe_allow_html=True,
)

# Display the user icon and text in the middle of the sidebar using a direct image URL
st.sidebar.markdown('<div class="user-icon-container"><div class="user-icon"><img src="https://dstudiosphotography.com/wp-content/uploads/2019/01/Corporate-Headshots-3-square.jpg" /></div><div>Jacob Smith</div><div>Jacob@business.com</div></div>', unsafe_allow_html=True)

st.sidebar.markdown('<br>', unsafe_allow_html=True) 
st.sidebar.markdown('<br>', unsafe_allow_html=True) 

add_selectbox = st.sidebar.selectbox(
    "Email",
    ("Compose", "Inbox", "Sent","Draft")
)

# Load your pre-trained model here
model_path = 'saved_model.pb'
tokenizer_encoder_path = "tokenizers.pkl"
tokenizer_decoder_path = "tokenizers.pkl"
embedding_matrix_enc_path = 'enc_embeddings.npy'
embedding_matrix_dec_path = 'dec_embeddings.npy'
max_len_enc = 16
max_len_dec = 5
vocab_size_enc = 5980
vocab_size_dec = 4573
embedding_size = 300 
lstm_units = 128

# Load your pre-trained model here

def load_model():
    try:
        loaded_model = tf.saved_model.load(model_path)
        return loaded_model
    except Exception as e:
        return None
    
def load_tokenizer(path, index=0):
    with open(path, 'rb') as tokenizer_file:
        tokenizers = pickle.load(tokenizer_file)
    return tokenizers[index]

def load_embedding_matrix(path):
    return np.load(path)

# Load your model and tokenizers
loaded_model = tf.keras.models.load_model(model_path)
if loaded_model is None:
    st.error("Error loading the model")
else:
    st.success("Model loaded successfully!")
    
tokenizer_encoder = load_tokenizer(tokenizer_encoder_path)
    
tokenizer_encoder = load_tokenizer(tokenizer_encoder_path, index=0)
tokenizer_decoder = load_tokenizer(tokenizer_decoder_path, index=1)
embedding_matrix_enc = load_embedding_matrix(embedding_matrix_enc_path)
embedding_matrix_dec = load_embedding_matrix(embedding_matrix_dec_path)


st.subheader('New Message', divider='grey')

# Input for the main recipient
receiver = st.text_input( "",'To: ')

# Checkbox for cc
cc_checkbox = st.checkbox('Cc')

# If the cc checkbox is selected, show a text input box for cc
if cc_checkbox:
    cc = st.text_input("",'Cc: ')

# Checkbox for bcc
bcc_checkbox = st.checkbox('Bcc')

# If the bcc checkbox is selected, show a text input box for bcc
if bcc_checkbox:
    bcc = st.text_input("",'Bcc: ')
    
    
subject = st.text_input("",'Subject: ')

MAX_SEQUENCE_LENGTH = max_len_enc  # Replace with the actual value

class Encoder(tf.keras.layers.Layer):
    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):
        super().__init__()
        self.lstm_size = lstm_size
        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length=input_length,
                                   mask_zero=True, name="embedding_layer_encoder", weights=[embedding_matrix_enc], trainable=True)
        self.lstm = LSTM(lstm_size, return_state=True, return_sequences=True, name="Encoder_LSTM")
        self.lstm_output = 0
        self.lstm_state_h = 0
        self.lstm_state_c = 0

    def call(self, input_sequence):
        input_embedd = self.embedding(input_sequence)
        self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.lstm(input_embedd)
        return self.lstm_output, self.lstm_state_h, self.lstm_state_c

encoder = Encoder(vocab_size_enc, embedding_size, lstm_units, max_len_enc)

def preprocess_input(input_text, tokenizer):
    input_tokens = tokenizer_encoder.texts_to_sequences([input_text])
    return pad_sequences(input_tokens, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

def predict(input_sentence,model):

  '''
  Given an input sentence, we need to predict the next words.
  Tokenize and pad the input sentence,
  Pass it to encoder's embedding layer and its output to encoder's LSTM layer.
  Pass its final states hidden and cell states as inputs to decoder's LSTM
  Before that, initialize  word and pass it to decoders embedding layer and then to LSTM layer,
  We run a loop till we get  word.
  We give predicted word at decoder's current step as input to its next step.
  '''

  encoder_test_tokens = tokenizer_encoder.texts_to_sequences([input_sentence])
  padded_encoder_input = pad_sequences(encoder_test_tokens, maxlen=16, dtype='float32', padding='post')
  encoder = model.layers[2]
  encoder_op, enc_h, enc_c = encoder(padded_encoder_input)
  decoder = model.layers[3]
  index_of_start = np.array(tokenizer_decoder.word_index['<start>']).reshape(1,1)
  states=(enc_h, enc_c)
  dense = model.layers[5]
  pred=0
  sentence = []
  while pred!=tokenizer_decoder.word_index['<end>']:
    dec_emb= decoder.embedding(index_of_start)
    predicted_out,state_h,state_c=decoder.lstm(dec_emb,states)
    pred = np.argmax(dense(predicted_out))
    word = [k for k in tokenizer_decoder.word_index if tokenizer_decoder.word_index[k]==pred][0]
    sentence.append(word)
    states= (state_h, state_c)
    index_of_start = np.array(pred).reshape(1,1)
  return ' '.join(sentence[:-1])

message = st.text_area("Message:")

if st.button("Predict Message"):
    if message:
        encoded_message = preprocess_input(message, tokenizer_encoder)
        predicted_message = predict(message, loaded_model)
        st.text_area("Predicted List is Here", predicted_message)
        
uploaded_file = st.file_uploader("Choose a file")
if uploaded_file is not None:
    # To read file as bytes:
    bytes_data = uploaded_file.getvalue()
    st.write(bytes_data)

    # To convert to a string based IO:
    stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
    st.write(stringio)

    # To read file as string:
    string_data = stringio.read()
    st.write(string_data)

    # Can be used wherever a "file-like" object is accepted:
    dataframe = pd.read_csv(uploaded_file)
    st.write(dataframe)


st.button("Send", type="primary")

